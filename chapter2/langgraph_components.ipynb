{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4824d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad42b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dotenv module\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding typing module for typed python\n",
    "# Documentation for python typing annotation https://docs.python.org/3/library/typing.html\n",
    "from typing import TypedDict, Annotated\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b20627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#langgraph module and tavily search tool\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, ToolMessage, HumanMessage\n",
    "from langchain_tavily import TavilySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb9d136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Gemini used for this example : https://python.langchain.com/docs/integrations/providers/google/\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chat_models import init_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d28e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_tavily.tavily_search.TavilySearch'>\n",
      "tavily_search\n",
      "{'name': 'tavily_search', 'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. It not only retrieves URLs and snippets, but offers advanced search depths, domain management, time range filters, and image search, this tool delivers real-time, accurate, and citation-backed results.Input should be a search query.', 'args_schema': <class 'langchain_tavily.tavily_search.TavilySearchInput'>, 'return_direct': False, 'verbose': False, 'callbacks': None, 'callback_manager': None, 'tags': None, 'metadata': None, 'handle_tool_error': True, 'handle_validation_error': False, 'response_format': 'content', 'auto_parameters': None, 'include_domains': None, 'exclude_domains': None, 'search_depth': None, 'include_images': None, 'time_range': None, 'max_results': 2, 'topic': None, 'include_answer': None, 'include_raw_content': None, 'include_image_descriptions': None, 'country': None, 'api_wrapper': TavilySearchAPIWrapper(tavily_api_key=SecretStr('**********'))}\n",
      "{'query': 'What is LangGraph?', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'What is LangGraph? - IBM', 'url': 'https://www.ibm.com/think/topics/langgraph', 'content': 'LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent’s state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. Nodes: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.', 'score': 0.9554898, 'raw_content': None}, {'title': 'What is LangGraph? - GeeksforGeeks', 'url': 'https://www.geeksforgeeks.org/machine-learning/what-is-langgraph/', 'content': 'LangGraph is a Python library that helps you build applications like chatbots or AI agents by organizing their logic step-by-step using state machine model. This step configures your Gemini API key and then we create a simple function ask_gemini that takes user input, sends it to the Gemini model and returns the AI-generated response. Creates a state structure with three fields: question, classification and response which flows through the LangGraph. import matplotlib.pyplot as plt from langgraph.graph import StateGraph\\u200bbuilder = StateGraph(GraphState)builder.add_node(\"classify\", classify)builder.add_node(\"respond\", respond)builder.set_entry_point(\"classify\")builder.add_edge(\"classify\", \"respond\")builder.set_finish_point(\"respond\")app = builder.compile()\\u200bdef visualize_workflow(builder): G = nx.DiGraph()\\u200b for node in builder.nodes: G.add_node(node) for edge in builder.edges: G.add_edge(edge[0], edge[1])\\u200b pos = nx.spring_layout(G) nx.draw(G, pos, with_labels=True, node_size=3000, node_color=\"skyblue\", font_size=12, font_weight=\"bold\", arrows=True)  plt.title(\"Langchain Workflow Visualization\") plt.show()\\u200bvisualize_workflow(builder)', 'score': 0.92260116, 'raw_content': None}], 'response_time': 1.51}\n",
      "{'query': 'What is LangGraph?', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'title': 'What is LangGraph? - IBM', 'url': 'https://www.ibm.com/think/topics/langgraph', 'content': 'LangGraph, created by LangChain, is an open source AI agent framework designed to build, deploy and manage complex generative AI agent workflows. At its core, LangGraph uses the power of graph-based architectures to model and manage the intricate relationships between various components of an AI agent workflow. LangGraph illuminates the processes within an AI workflow, allowing full transparency of the agent’s state. By combining these technologies with a set of APIs and tools, LangGraph provides users with a versatile platform for developing AI solutions and workflows including chatbots, state graphs and other agent-based systems. Nodes: In LangGraph, nodes represent individual components or agents within an AI workflow. LangGraph uses enhanced decision-making by modeling complex relationships between nodes, which means it uses AI agents to analyze their past actions and feedback.', 'score': 0.9554898, 'raw_content': None}, {'title': 'What is LangGraph? - GeeksforGeeks', 'url': 'https://www.geeksforgeeks.org/machine-learning/what-is-langgraph/', 'content': 'LangGraph is a Python library that helps you build applications like chatbots or AI agents by organizing their logic step-by-step using state machine model. This step configures your Gemini API key and then we create a simple function ask_gemini that takes user input, sends it to the Gemini model and returns the AI-generated response. Creates a state structure with three fields: question, classification and response which flows through the LangGraph. import matplotlib.pyplot as plt from langgraph.graph import StateGraph\\u200bbuilder = StateGraph(GraphState)builder.add_node(\"classify\", classify)builder.add_node(\"respond\", respond)builder.set_entry_point(\"classify\")builder.add_edge(\"classify\", \"respond\")builder.set_finish_point(\"respond\")app = builder.compile()\\u200bdef visualize_workflow(builder): G = nx.DiGraph()\\u200b for node in builder.nodes: G.add_node(node) for edge in builder.edges: G.add_edge(edge[0], edge[1])\\u200b pos = nx.spring_layout(G) nx.draw(G, pos, with_labels=True, node_size=3000, node_color=\"skyblue\", font_size=12, font_weight=\"bold\", arrows=True)  plt.title(\"Langchain Workflow Visualization\") plt.show()\\u200bvisualize_workflow(builder)', 'score': 0.92260116, 'raw_content': None}], 'response_time': 1.51}\n"
     ]
    }
   ],
   "source": [
    "tool = TavilySearch(max_results=2)\n",
    "print(type(tool))\n",
    "print(tool.name)\n",
    "print(vars(tool))\n",
    "#response = tool.invoke(\"What is LangGraph?\")\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f51fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47fb763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, model, tools, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "       #self.model = model.bind_tools(tools)\n",
    "        self.model = model\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if not t['name'] in self.tools:      # check for bad tool name from LLM\n",
    "                print(\"\\n ....bad tool name....\")\n",
    "                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n",
    "            else:\n",
    "                result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e79d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "\n",
    "# model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "model = init_chat_model(\"google_genai:gemini-2.0-flash\").bind_tools([tool])\n",
    "\n",
    "\n",
    "abot = Agent(model, [tool], system=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3280819",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We can easily display the graph we have created\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(abot.graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db21778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content = \"What is the weather in sf?\")]\n",
    "result = abot.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0e6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50702b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b828ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in SF and LA?\")]\n",
    "result = abot.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['messages'][-1].content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
