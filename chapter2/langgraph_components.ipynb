{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4824d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad42b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dotenv module\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f6f628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding typing module for typed python\n",
    "# Documentation for python typing annotation https://docs.python.org/3/library/typing.html\n",
    "from typing import TypedDict, Annotated\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b20627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#langgraph module and tavily search tool\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, ToolMessage, HumanMessage\n",
    "from langchain_tavily import TavilySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb9d136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Gemini used for this example : https://python.langchain.com/docs/integrations/providers/google/\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chat_models import init_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d28e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = TavilySearch(max_results=2)\n",
    "print(type(tool))\n",
    "print(tool.name)\n",
    "print(vars(tool))\n",
    "#response = tool.invoke(\"What is LangGraph?\")\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f51fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47fb763",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, model, tools, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "       #self.model = model.bind_tools(tools)\n",
    "        self.model = model\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if not t['name'] in self.tools:      # check for bad tool name from LLM\n",
    "                print(\"\\n ....bad tool name....\")\n",
    "                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n",
    "            else:\n",
    "                result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e79d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "\n",
    "model = init_chat_model(\"google_genai:gemini-2.5-pro\").bind_tools([tool])\n",
    "\n",
    "\n",
    "abot = Agent(model, [tool], system=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3280819",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We can easily display the graph we have created\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(abot.graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db21778a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content = \"What is the weather in sf?\")]\n",
    "result = abot.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0e6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50702b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['messages'][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b828ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in SF and LA?\")]\n",
    "result = abot.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['messages'][-1].content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
