{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35506640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install required dependencies\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed4867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dotenv module\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ba478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding typing module for typed python\n",
    "# Documentation for python typing annotation https://docs.python.org/3/library/typing.html\n",
    "from typing import TypedDict, Annotated\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62507312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#langgraph module and tavily search tool\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, ToolMessage, HumanMessage, AIMessage\n",
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "from uuid import uuid4\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dd4825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Gemini used for this example : https://python.langchain.com/docs/integrations/providers/google/\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "tool = TavilySearch(max_results=2)\n",
    "model = init_chat_model(\"google_genai:gemini-2.5-pro\").bind_tools([tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d490b8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In previous examples we've annotated the `messages` state key \n",
    "with the default `operator.add` or `+` reducer, which always \n",
    "appends new messages to the end of the existing messages array.\n",
    "\n",
    "Now, to support replacing existing messages, we annotate the\n",
    "`messages` key with a customer reducer function, which replaces\n",
    "messages with the same `id`, and appends them otherwise.\n",
    "\"\"\"\n",
    "def reduce_messages(left: list[AnyMessage], right: list[AnyMessage]) -> list [AnyMessage]:\n",
    "    # assign ids to messages that don't have them\n",
    "    for message in right:\n",
    "        if not message.id:\n",
    "            message.id = str(uuid4())\n",
    "    # merge the new messages with the existing messages\n",
    "    merged = left.copy()\n",
    "    for message in right:\n",
    "        for i, existing in enumerate(merged):\n",
    "            # replace any existing messages with the same id\n",
    "            if existing.id == message.id:\n",
    "                merged[i] = message\n",
    "                break\n",
    "        else:\n",
    "            # append any new messages to the end\n",
    "            merged.append(message)\n",
    "    return merged\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], reduce_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814c22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        # The interrupt before action makes to stop the node before executing any tool\n",
    "        self.graph = graph.compile(checkpointer = checkpointer,\n",
    "                                   interrupt_before=[\"action\"])\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            if not t['name'] in self.tools:      # check for bad tool name from LLM\n",
    "                print(\"\\n ....bad tool name....\")\n",
    "                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n",
    "            else:\n",
    "                result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cea1eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information.\n",
    "You are allowed to make multiple calls (either together or in sequence).\n",
    "Only look up information when you are sure of what you want.\n",
    "If you need to look up some information before asking a follow un question, you are allowed to do that!\"\"\"\n",
    "\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb839b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this, the loop stops just before calling tavily seach api\n",
    "messages = [HumanMessage(content=\"How is the weather in SF?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635e9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "abot.graph.get_state(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2582f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see the next stage of the graph\n",
    "abot.graph.get_state(thread).next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544d22f8",
   "metadata": {},
   "source": [
    "## Continue after the stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5583dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's enough just adding None as a first parameter\n",
    "for event in abot.graph.stream(None, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb89cb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "abot.graph.get_state(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76046b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "abot.graph.get_state(thread).next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(\"Whats the weather in Los Angeles?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)\n",
    "while abot.graph.get_state(thread).next:\n",
    "    print(\"\\n\", abot.graph.get_state(thread),\"\\n\")\n",
    "    _input = input(\"proceed?\")\n",
    "    if _input != \"y\":\n",
    "        print(\"aborting\")\n",
    "        break\n",
    "    for event in abot.graph.stream(None, thread):\n",
    "        for v in event.values():\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6bb0c",
   "metadata": {},
   "source": [
    "## Modify State\n",
    "\n",
    "Run until the interrupt and then, modify the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2292c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [HumanMessage(\"Whats the weather in LA?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f814f801",
   "metadata": {},
   "outputs": [],
   "source": [
    "abot.graph.get_state(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9be406",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_values = abot.graph.get_state(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56007b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_values.values['messages'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94bf37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_values.values['messages'][-1].tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_id = current_values.values['messages'][-1].tool_calls[0]['id']\n",
    "current_values.values['messages'][-1].tool_calls = [\n",
    "    {'name': 'tavily_search_results_json',\n",
    "  'args': {'query': 'current weather in Louisiana'},\n",
    "  'id': _id}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063aad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "abot.graph.update_state(thread, current_values.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e518b58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "abot.graph.get_state(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c156f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in abot.graph.stream(None, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc985a6",
   "metadata": {},
   "source": [
    "## Time Travel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
